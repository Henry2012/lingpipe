[文本分类](http://blog.csdn.net/luowen3405/article/details/6420526)
===
* 不同于网页分类
* 核心问题：用哪些特征表示一个文本才能保证有效和快速的分类（这两个需求往往是互相矛盾的）。
* 统计学习方法
	* 建立一个文档表示模型(representation)
	* 假设：文档的内容与其中包含的词有着必然的联系，同一类文档之间总存在多个共同的词，而不同类的文档所包含的词之间差异很大
	* 不仅仅是包含的词，还有这些词出现的次数对分类也很重要 --> 向量空间模型（Vector Space Modeling） --> 一篇文章被看作特征集合来看，利用加权特征项构成向量进行文本表示，利用词频信息对文本特征进行加权 --> VSM基本上完全忽略了除词的信息以外所有的部分，比如词之间顺序关系带来的信息以及上下文信息等，这使得它能表达的信息量存在__上限__ --> __基于Bag of words模型上的建模__ --> 有益的尝试，LSI（潜在语义索引）被实验证明保留了一定的语义信息
* 训练
	* TF/IDF
	* 去除停止词 （Stopwords）
* some issues in text classification
	* 高维性
	* 向量稀疏性
	
	
	